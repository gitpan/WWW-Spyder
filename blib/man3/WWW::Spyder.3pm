.\" Automatically generated by Pod::Man v1.37, Pod::Parser v1.14
.\"
.\" Standard preamble:
.\" ========================================================================
.de Sh \" Subsection heading
.br
.if t .Sp
.ne 5
.PP
\fB\\$1\fR
.PP
..
.de Sp \" Vertical space (when we can't use .PP)
.if t .sp .5v
.if n .sp
..
.de Vb \" Begin verbatim text
.ft CW
.nf
.ne \\$1
..
.de Ve \" End verbatim text
.ft R
.fi
..
.\" Set up some character translations and predefined strings.  \*(-- will
.\" give an unbreakable dash, \*(PI will give pi, \*(L" will give a left
.\" double quote, and \*(R" will give a right double quote.  | will give a
.\" real vertical bar.  \*(C+ will give a nicer C++.  Capital omega is used to
.\" do unbreakable dashes and therefore won't be available.  \*(C` and \*(C'
.\" expand to `' in nroff, nothing in troff, for use with C<>.
.tr \(*W-|\(bv\*(Tr
.ds C+ C\v'-.1v'\h'-1p'\s-2+\h'-1p'+\s0\v'.1v'\h'-1p'
.ie n \{\
.    ds -- \(*W-
.    ds PI pi
.    if (\n(.H=4u)&(1m=24u) .ds -- \(*W\h'-12u'\(*W\h'-12u'-\" diablo 10 pitch
.    if (\n(.H=4u)&(1m=20u) .ds -- \(*W\h'-12u'\(*W\h'-8u'-\"  diablo 12 pitch
.    ds L" ""
.    ds R" ""
.    ds C` ""
.    ds C' ""
'br\}
.el\{\
.    ds -- \|\(em\|
.    ds PI \(*p
.    ds L" ``
.    ds R" ''
'br\}
.\"
.\" If the F register is turned on, we'll generate index entries on stderr for
.\" titles (.TH), headers (.SH), subsections (.Sh), items (.Ip), and index
.\" entries marked with X<> in POD.  Of course, you'll have to process the
.\" output yourself in some meaningful fashion.
.if \nF \{\
.    de IX
.    tm Index:\\$1\t\\n%\t"\\$2"
..
.    nr % 0
.    rr F
.\}
.\"
.\" For nroff, turn off justification.  Always turn off hyphenation; it makes
.\" way too many mistakes in technical documents.
.hy 0
.if n .na
.\"
.\" Accent mark definitions (@(#)ms.acc 1.5 88/02/08 SMI; from UCB 4.2).
.\" Fear.  Run.  Save yourself.  No user-serviceable parts.
.    \" fudge factors for nroff and troff
.if n \{\
.    ds #H 0
.    ds #V .8m
.    ds #F .3m
.    ds #[ \f1
.    ds #] \fP
.\}
.if t \{\
.    ds #H ((1u-(\\\\n(.fu%2u))*.13m)
.    ds #V .6m
.    ds #F 0
.    ds #[ \&
.    ds #] \&
.\}
.    \" simple accents for nroff and troff
.if n \{\
.    ds ' \&
.    ds ` \&
.    ds ^ \&
.    ds , \&
.    ds ~ ~
.    ds /
.\}
.if t \{\
.    ds ' \\k:\h'-(\\n(.wu*8/10-\*(#H)'\'\h"|\\n:u"
.    ds ` \\k:\h'-(\\n(.wu*8/10-\*(#H)'\`\h'|\\n:u'
.    ds ^ \\k:\h'-(\\n(.wu*10/11-\*(#H)'^\h'|\\n:u'
.    ds , \\k:\h'-(\\n(.wu*8/10)',\h'|\\n:u'
.    ds ~ \\k:\h'-(\\n(.wu-\*(#H-.1m)'~\h'|\\n:u'
.    ds / \\k:\h'-(\\n(.wu*8/10-\*(#H)'\z\(sl\h'|\\n:u'
.\}
.    \" troff and (daisy-wheel) nroff accents
.ds : \\k:\h'-(\\n(.wu*8/10-\*(#H+.1m+\*(#F)'\v'-\*(#V'\z.\h'.2m+\*(#F'.\h'|\\n:u'\v'\*(#V'
.ds 8 \h'\*(#H'\(*b\h'-\*(#H'
.ds o \\k:\h'-(\\n(.wu+\w'\(de'u-\*(#H)/2u'\v'-.3n'\*(#[\z\(de\v'.3n'\h'|\\n:u'\*(#]
.ds d- \h'\*(#H'\(pd\h'-\w'~'u'\v'-.25m'\f2\(hy\fP\v'.25m'\h'-\*(#H'
.ds D- D\\k:\h'-\w'D'u'\v'-.11m'\z\(hy\v'.11m'\h'|\\n:u'
.ds th \*(#[\v'.3m'\s+1I\s-1\v'-.3m'\h'-(\w'I'u*2/3)'\s-1o\s+1\*(#]
.ds Th \*(#[\s+2I\s-2\h'-\w'I'u*3/5'\v'-.3m'o\v'.3m'\*(#]
.ds ae a\h'-(\w'a'u*4/10)'e
.ds Ae A\h'-(\w'A'u*4/10)'E
.    \" corrections for vroff
.if v .ds ~ \\k:\h'-(\\n(.wu*9/10-\*(#H)'\s-2\u~\d\s+2\h'|\\n:u'
.if v .ds ^ \\k:\h'-(\\n(.wu*10/11-\*(#H)'\v'-.4m'^\v'.4m'\h'|\\n:u'
.    \" for low resolution devices (crt and lpr)
.if \n(.H>23 .if \n(.V>19 \
\{\
.    ds : e
.    ds 8 ss
.    ds o a
.    ds d- d\h'-1'\(ga
.    ds D- D\h'-1'\(hy
.    ds th \o'bp'
.    ds Th \o'LP'
.    ds ae ae
.    ds Ae AE
.\}
.rm #[ #] #H #V #F C
.\" ========================================================================
.\"
.IX Title "Spyder 3"
.TH Spyder 3 "2007-09-01" "perl v5.8.6" "User Contributed Perl Documentation"
.SH "NAME"
WWW::Spyder \- a simple non\-persistent web crawler.
.SH "VERSION 0.20"
.IX Header "VERSION 0.20"
.SH "SYNOPSIS"
.IX Header "SYNOPSIS"
A web spider that returns plain text, \s-1HTML\s0, and other information per
page crawled and can determine what pages to get and parse based on
supplied terms compared to the text in links as well as page content.
.SH "METHODS"
.IX Header "METHODS"
.ie n .IP "* $spyder\fR\->\fInew()" 2
.el .IP "* \f(CW$spyder\fR\->\fInew()\fR" 2
.IX Item "$spyder->new()"
Construct a new spyder object. Without at least the \fIseed()\fR set, or
\&\fIgo_to_seed()\fR turned on, the spyder isn't ready to crawl.
.Sp
.Vb 3
\& $spyder = WWW::Spyder->new(shift||die"Gimme a URL!\en");
\&    # ...or...
\& $spyder = WWW::Spyder->new( %options );
.Ve
.Sp
Options include: sleep_base (in seconds), exit_on (hash of methods and
settings). Examples below.
.ie n .IP "* $spyder\->seed($url)" 2
.el .IP "* \f(CW$spyder\fR\->seed($url)" 2
.IX Item "$spyder->seed($url)"
Adds a \s-1URL\s0 (or URLs) to the top of the queues for crawling. If the
spyder is constructed with a single scalar argument, that is considered
the seed.
.ie n .IP "* $spyder\->bell([bool])" 2
.el .IP "* \f(CW$spyder\fR\->bell([bool])" 2
.IX Item "$spyder->bell([bool])"
This will print a bell (\*(L"\ea\*(R") to \s-1STDERR\s0 on every successfully crawled
page. It might seem annoying but it is an excellent way to know your
spyder is behaving and working. True value turns it on. Right now it
can't be turned off.
.ie n .IP "* $spyder\->spyder_time([bool])" 2
.el .IP "* \f(CW$spyder\fR\->spyder_time([bool])" 2
.IX Item "$spyder->spyder_time([bool])"
Returns raw seconds since \fISpyder\fR was created if given a
boolean value, otherwise returns \*(L"D day(s) \s-1HH::MM:SS\s0.\*(R"
.ie n .IP "* $spyder\->terms([list of terms to match])" 2
.el .IP "* \f(CW$spyder\fR\->terms([list of terms to match])" 2
.IX Item "$spyder->terms([list of terms to match])"
The more terms, the more the spyder is going to grasp at. If you give
a straight list of strings, they will be turned into very open
regexes. E.g.: \*(L"king\*(R" would match \*(L"sulking\*(R" and \*(L"kinglet\*(R" but not
\&\*(L"King.\*(R" It is case sensitive right now. If you want more specific
matching or different behavior, pass your own regexes instead of
strings.
.Sp
.Vb 1
\&    $spyder->terms( qr/\ebkings?\eb/i, qr/\ebqueens?\eb/i );
.Ve
.Sp
\&\fIterms()\fR is only settable once right now, then it's a done deal.
.ie n .IP "* $spyder\fR\->\fIspyder_data()" 2
.el .IP "* \f(CW$spyder\fR\->\fIspyder_data()\fR" 2
.IX Item "$spyder->spyder_data()"
A comma formatted number of kilobytes retrieved so far. \fBDon't\fR give
it an argument. It's a set/get routine.
.ie n .IP "* $spyder\fR\->\fIslept()" 2
.el .IP "* \f(CW$spyder\fR\->\fIslept()\fR" 2
.IX Item "$spyder->slept()"
Returns the total number of seconds the spyder has slept while
running. Useful for getting accurate page/time counts (spyder
performance) discounting the added courtesy naps.
.ie n .IP "* $spyder\->\s-1UA\-\s0>..." 2
.el .IP "* \f(CW$spyder\fR\->\s-1UA\-\s0>..." 2
.IX Item "$spyder->UA->..."
The LWP::UserAgent. You can reset them, I do believe, by calling
methods on the \s-1UA\s0. Here are the initialized values you might want to
tweak (see LWP::UserAgent for more information):
.Sp
.Vb 3
\&    $spyder->UA->timeout(30);
\&    $spyder->UA->max_size(250_000);
\&    $spyder->UA->agent('Mozilla/5.0');
.Ve
.Sp
Changing the agent name can hurt your spyder b/c some servers won't
return content unless it's requested by a \*(L"browser\*(R" they recognize.
.Sp
You should probably add your email with \fIfrom()\fR as well.
.Sp
.Vb 1
\&    $spyder->UA->from('bluefintuna@fish.net');
.Ve
.ie n .IP "* $spyder\->cookie_file([local_file])" 2
.el .IP "* \f(CW$spyder\fR\->cookie_file([local_file])" 2
.IX Item "$spyder->cookie_file([local_file])"
They live in \f(CW$ENV\fR{\s-1HOME\s0}/spyderCookie by default but you can set your
own file if you prefer or want to save different cookie files for
different spyders.
.Sh "Weird courteous behavior"
.IX Subsection "Weird courteous behavior"
Courtesy didn't used to be weird, but that's another story. You will
probably notice that the courtesy routines force a sleep when a
recently seen domain is the only choice for a new link. The sleep is
partially randomized. This is to prevent the spyder from being
recognized in weblogs as a robot.
.Sh "The web and courtesy"
.IX Subsection "The web and courtesy"
\&\fBPlease\fR, I beg of thee, exercise the most courtesy you can. Don't
let impatience get in the way. Bandwidth and server traffic are
\&\f(CW$MONEY\fR for real. The web is an extremely disorganized and corrupted
database at the root but companies and individuals pay to keep it
available. The less pain you cause by banging away on a webserver with
a web agent, the more welcome the next web agent will be.
.Sp
\&\fBUpdate\fR: Google seems to be excluding generic \s-1LWP\s0 agents now. See, I
told you so. A single parallel robot can really hammer a major server,
even someone with as big a farm and as much bandwidth as Google.
.Sh "\s-1VERBOSITY\s0"
.IX Subsection "VERBOSITY"
.RS 2
.ie n .IP "* $spyder\->verbosity([1\-6])  \-OR\-" 2
.el .IP "* \f(CW$spyder\fR\->verbosity([1\-6])  \-OR\-" 2
.IX Item "$spyder->verbosity([1-6])  -OR-"
.PD 0
.ie n .IP "* $WWW::Spyder::VERBOSITY = ..." 2
.el .IP "* \f(CW$WWW::Spyder::VERBOSITY\fR = ..." 2
.IX Item "$WWW::Spyder::VERBOSITY = ..."
.PD
Set it from 1 to 6 right now to get varying amounts of extra info to
\&\s-1STDOUT\s0. It's an uneven scale and will be straightened out pretty soon.
If kids have a preference for sending the info to \s-1STDERR\s0, I'll do
that. I might anyway.
.RE
.RS 2
.SH "SAMPLE USAGE"
.IX Header "SAMPLE USAGE"
.ie n .Sh "See ""spyder\-mini\-bio"" in this distribution"
.el .Sh "See ``spyder\-mini\-bio'' in this distribution"
.IX Subsection "See spyder-mini-bio in this distribution"
It's an extremely simple, but fairly cool pseudo bio\-researcher.
.Sh "Simple continually crawling spyder:"
.IX Subsection "Simple continually crawling spyder:"
In the following code snippet:
.Sp
.Vb 1
\& use WWW::Spyder;
.Ve
.Sp
.Vb 1
\& my $spyder = WWW::Spyder->new( shift || die"Give me a URL!\en" );
.Ve
.Sp
.Vb 1
\& while ( my $page = $spyder->crawl ) {
.Ve
.Sp
.Vb 13
\&    print '-'x70,"\en";
\&    print "Spydering: ", $page->title, "\en";
\&    print "      URL: ", $page->url, "\en";
\&    print "     Desc: ", $page->description || 'n/a', "\en";
\&    print '-'x70,"\en";
\&    while ( my $link = $page->next_link ) {
\&        printf "%22s ->> %s\en",
\&        length($link->name) > 22 ?
\&            substr($link->name,0,19).'...' : $link->name,
\&            length($link) > 43 ?
\&                substr($link,0,40).'...' : $link;
\&    }
\& }
.Ve
.Sp
as long as unique URLs are being found in the pages crawled, the
spyder will never stop.
.Sp
Each \*(L"crawl\*(R" returns a page object which gives the following methods
to get information about the page.
.ie n .IP "* $page\->links" 2
.el .IP "* \f(CW$page\fR\->links" 2
.IX Item "$page->links"
URLs found on the page.
.ie n .IP "* $page\->title" 2
.el .IP "* \f(CW$page\fR\->title" 2
.IX Item "$page->title"
Page's <\s-1TITLE\s0> Title </TITLE> if there is one.
.ie n .IP "* $page\->text" 2
.el .IP "* \f(CW$page\fR\->text" 2
.IX Item "$page->text"
The parsed plain text out of the page. Uses HTML::Parser and tries to
ignore non-readable stuff like comments and scripts.
.ie n .IP "* $page\->url" 2
.el .IP "* \f(CW$page\fR\->url" 2
.IX Item "$page->url"
.PD 0
.ie n .IP "* $page\->domain" 2
.el .IP "* \f(CW$page\fR\->domain" 2
.IX Item "$page->domain"
.ie n .IP "* $page\->raw" 2
.el .IP "* \f(CW$page\fR\->raw" 2
.IX Item "$page->raw"
.PD
The content returned by the server. Should be \s-1HTML\s0.
.ie n .IP "* $page\->description" 2
.el .IP "* \f(CW$page\fR\->description" 2
.IX Item "$page->description"
The \s-1META\s0 description of the page if there is one.
.ie n .IP "* $page\->links" 2
.el .IP "* \f(CW$page\fR\->links" 2
.IX Item "$page->links"
Returns a list of the URLs in the page. Note: \fInext_link()\fR will shift
the available list of \fIlinks()\fR each time it's called.
.ie n .IP "* $link\fR = \f(CW$page\->next_link" 2
.el .IP "* \f(CW$link\fR = \f(CW$page\fR\->next_link" 2
.IX Item "$link = $page->next_link"
\&\fInext_link()\fR destructively returns the next URI-ish object in the page.
They are objects with three accessors.
.RS 2
.ie n .IP "* $link\->url" 10
.el .IP "* \f(CW$link\fR\->url" 10
.IX Item "$link->url"
This is also overloaded so that interpolating \*(L"$link\*(R" will get the
\&\s-1URL\s0 just as the method does.
.ie n .IP "* $link\->name" 10
.el .IP "* \f(CW$link\fR\->name" 10
.IX Item "$link->name"
.PD 0
.ie n .IP "* $link\->domain" 10
.el .IP "* \f(CW$link\fR\->domain" 10
.IX Item "$link->domain"
.RE
.RS 2
.RE
.RE
.RS 2
.PD
.Sh "Spyder that will give up the ghost..."
.IX Subsection "Spyder that will give up the ghost..."
The following spyder is initialized to stop crawling when \fIeither\fR of
its conditions are met: 10mins pass or 300 pages are crawled.
.Sp
.Vb 1
\& use WWW::Spyder;
.Ve
.Sp
.Vb 1
\& my $url = shift || die "Please give me a URL to start!\en";
.Ve
.Sp
.Vb 5
\& my $spyder = WWW::Spyder->new
\&      (seed        => $url,
\&       sleep_base  => 10,
\&       exit_on     => { pages => 300,
\&                        time  => '10min', },);
.Ve
.Sp
.Vb 1
\& while ( my $page = $spyder->crawl ) {
.Ve
.Sp
.Vb 13
\&    print '-'x70,"\en";
\&    print "Spydering: ", $page->title, "\en";
\&    print "      URL: ", $page->url, "\en";
\&    print "     Desc: ", $page->description || '', "\en";
\&    print '-'x70,"\en";
\&    while ( my $link = $page->next_link ) {
\&        printf "%22s ->> %s\en",
\&        length($link->name) > 22 ?
\&            substr($link->name,0,19).'...' : $link->name,
\&            length($link) > 43 ?
\&                substr($link,0,40).'...' : $link;
\&    }
\& }
.Ve
.Sh "Primitive page reader"
.IX Subsection "Primitive page reader"
.Vb 2
\& use WWW::Spyder;
\& use Text::Wrap;
.Ve
.Sp
.Vb 5
\& my $url = shift || die "Please give me a URL to start!\en";
\& @ARGV or die "Please also give me a search term.\en";
\& my $spyder = WWW::Spyder->new;
\& $spyder->seed($url);
\& $spyder->terms(@ARGV);
.Ve
.Sp
.Vb 7
\& while ( my $page = $spyder->crawl ) {
\&     print '-'x70,"\en * ";
\&     print $page->title, "\en";
\&     print '-'x70,"\en";
\&     print wrap('','', $page->text);
\&     sleep 60;
\& }
.Ve
.SH "TIPS"
.IX Header "TIPS"
If you are going to do anything important with it, implement some
signal blocking to prevent accidental problems and tie your gathered
information to a DB_File or some such.
.Sp
You might want to load \f(CW\*(C`POSIX::nice(40)\*(C'\fR. It should top the nice off
at your system's max and prevent your spyder from interfering with
your system.
.Sp
You might want to to set $| = 1.
.SH "PRIVATE METHODS"
.IX Header "PRIVATE METHODS"
.Sh "are private but hack away if you're inclined"
.IX Subsection "are private but hack away if you're inclined"
.SH "TO DO"
.IX Header "TO DO"
\&\fISpyder\fR is conceived to live in a future namespace as a servant class
for a complex web research agent with simple interfaces to
pre-designed grammars for research reports; or self-designed
grammars/reports (might be implemented via Parse::FastDescent if that
lazy-bones Conway would just find another 5 hours in the paltry 32
hour day he's presently working).
.Sp
I'd like the thing to be able to parse \s-1RTF\s0, \s-1PDF\s0, and perhaps even
resource sections of image files but that isn't on the radar right
now.
.Sp
The tests should work differently. Currently they ask for outside
resources without checking if there is either an open way to do it or
if the user approves of it. Bad form all around.
.SH "TO DOABLE BY 1.0"
.IX Header "TO DOABLE BY 1.0"
Add 2\-4 sample scripts that are a bit more useful.
.Sp
There are many functions that should be under the programmer's control
and not buried in the spyder. They will emerge soon. I'd like to put
in hooks to allow the user to \fIkeep()\fR, \fItoss()\fR, or \fIexclude()\fR, urls, link
names, and domains, while crawling.
.Sp
Clean up some redundant, sloppy, and weird code. Probably change or
remove the \s-1AUTOLOAD\s0.
.Sp
Put in a \fIgo_to_seed()\fR method and a subclass, ::Seed, with rules to
construct query URLs by search engine. It would be the autostart or the
fallback for perpetual spyders that run out of links. It would hit a
given or default search engine with the \fISpyder\fR's terms as the query.
Obviously this would only work with \fIterms()\fR defined.
.Sp
Implement auto-exclusion for failure vs. success rates on names as well
as domains (maybe \s-1URI\s0 suffixes too).
.Sp
Turn length of courtesy queue into the breadth/depth setting? make it
automatically adjusting...?
.Sp
Consistently found link names are excluded from term strength sorting?
Eg: \*(L"privacy policy,\*(R" \*(L"read more,\*(R" \*(L"copyright...\*(R"
.Sp
Fix some image tag parsing problems and add area tag parsing.
.Sp
Configuration for user:password by domain.
.Sp
::Page objects become reusable so that a spyder only needs one.
.Sp
::Enqueue objects become indexed so they are nixable from anywhere.
.Sp
Expand exit_on routines to size, slept time, dwindling success ratio,
and maybe more.
.Sp
Make methods to set \*(L"skepticism\*(R" and \*(L"effort\*(R" which will influence the
way the terms are used to keep, order, and toss URLs.
.SH "BE WARNED"
.IX Header "BE WARNED"
This module already does some extremely useful things but it's in its
infancy and it is conceived to live in a different namespace and
perhaps become more private as a subservient part of a parent class.
This may never happen but it's the idea. So don't put this into
production code yet. I am endeavoring to keep its interface constant
either way. That said, it could change completely.
.Sh "Also!"
.IX Subsection "Also!"
This module saves cookies to the user's home. There will be more
control over cookies in the future, but that's how it is right now.
They live in \f(CW$ENV\fR{\s-1HOME\s0}/spyderCookie.
.Sh "Anche!"
.IX Subsection "Anche!"
Robot Rules aren't respected. \fISpyder\fR endeavors to be polite as far
as server hits are concerned, but doesn't take \*(L"no\*(R" for answer right
now. I want to add this, and not just by domain, but by page settings.
.SH "UNDOCUMENTED FEATURES"
.IX Header "UNDOCUMENTED FEATURES"
A.k.a. Bugs. Don't be ridiculous! Bugs in \fBmy code\fR?! 
.Sp
There is a bug that is causing retrieval of image src tags, I think
but haven't tracked it down yet, as links. I also think the plain text
parsing has some problems which will be remedied shortly.
.Sp
If you are building more than one spyder in the same script they are
going to share the same exit_on parameters because it's a
self-installing method. This will not always be so.
.Sp
See \fBBugs\fR file for more open and past issues.
.Sp
Let me know if you find any others. If you find one that is platform
specific, please send patch code/suggestion b/c I might not have any
idea how to fix it.
.ie n .SH "WHY ""Spyder?"""
.el .SH "WHY \f(CWSpyder?\fP"
.IX Header "WHY Spyder?"
I didn't want to use the more appropriate \fISpider\fR because I think
there is a better one out there somewhere in the zeitgeist and the
namespace future of \fISpyder\fR is uncertain. It may end up a
semi-private part of a bigger family. And I may be King of Kenya
someday. One's got to dream.
.Sp
If you like \fISpyder\fR, have feedback, wishlist usage, better
algorithms/implementations for any part of it, please let me know!
.SH "AUTHOR, AUTHOR"
.IX Header "AUTHOR, AUTHOR"
Ashley5, ashley@cpan.org. Bob's your monkey's uncle.
.SH "COPYRIGHT"
.IX Header "COPYRIGHT"
(c)2001\-2002 Ashley Pond V. All rights reserved. This program is free
software; you may redistribute or modify it under the same terms as
Perl.
.SH "THANKS TO"
.IX Header "THANKS TO"
Most all y'all. Especially Lincoln Stein, Gisle Aas, The Conway,
Raphael Manfredi, Gurusamy Sarathy, and plenty of others.
.SH "COMPARE WITH (PROBABLY PREFER)"
.IX Header "COMPARE WITH (PROBABLY PREFER)"
WWW::Robot, LWP::UserAgent, WWW::SimpleRobot, WWW::RobotRules,
LWP::RobotUA, and other kith and kin.
